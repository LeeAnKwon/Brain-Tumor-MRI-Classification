{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhVyyujST9FS"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import math  # ceil 사용을 위해 추가\n",
        "\n",
        "\n",
        "# 1) 하이퍼파라미터 설정\n",
        "\n",
        "batch_size = 64            # 한 배치에 처리할 이미지 수\n",
        "epochs = 50                # 전체 학습 반복 횟수\n",
        "learning_rate = 0.001      # 초기 학습률\n",
        "input_size = 128           # 이미지 가로 픽셀 수 (RNN input feature 차원)\n",
        "sequence_length = 128      # 이미지 세로 픽셀 수 (RNN time step 수)\n",
        "hidden_size = 256          # RNN 은닉 상태 크기\n",
        "num_layers = 3             # RNN 레이어 개수 (stacked RNN)\n",
        "num_classes = 4            # 분류할 클래스 개수 (종양 종류 수)\n",
        "\n",
        "\n",
        "# 2) 데이터 증강 및 전처리 설정\n",
        "\n",
        "data_dir = '/content/BrainMRI'\n",
        "assert os.path.exists(data_dir), \"데이터 폴더가 존재하지 않습니다.\"\n",
        "\n",
        "# ImageDataGenerator를 이용한 데이터 증강 및 정규화\n",
        "train_gen = ImageDataGenerator(\n",
        "    rescale=1./255,            # 픽셀 값을 [0,1] 범위로 정규화\n",
        "    validation_split=0.3,      # 전체 데이터의 30%를 검증용으로 분리\n",
        "    rotation_range=30,         # 랜덤 회전 최대 30도\n",
        "    zoom_range=0.2,            # 랜덤 줌 최대 20%\n",
        "    width_shift_range=0.2,     # 좌우 이동 최대 20%\n",
        "    height_shift_range=0.2,    # 상하 이동 최대 20%\n",
        "    horizontal_flip=True       # 좌우 반전 허용\n",
        ")\n",
        "\n",
        "val_gen = ImageDataGenerator(\n",
        "    rescale=1./255,            # 검증 데이터는 증강 없이 정규화만 적용\n",
        "    validation_split=0.3\n",
        ")\n",
        "\n",
        "# 증강 포함 학습 데이터 생성기\n",
        "train_generator = train_gen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(input_size, sequence_length),  # 이미지 크기 맞춤 (128x128)\n",
        "    color_mode='grayscale',                      # 흑백 이미지 (1채널)\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',                     # 다중 클래스 원-핫 인코딩 라벨\n",
        "    subset='training',                            # 학습용 데이터 분할\n",
        "    shuffle=True                                 # 매 epoch마다 셔플\n",
        ")\n",
        "\n",
        "# 검증 데이터 생성기\n",
        "val_generator = val_gen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(input_size, sequence_length),\n",
        "    color_mode='grayscale',\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',                          # 검증용 데이터 분할\n",
        "    shuffle=False                                # 검증 시 셔플 불필요\n",
        ")\n",
        "\n",
        "\n",
        "# 3) RNN 기반 모델 구성\n",
        "\n",
        "# 입력 형태: (batch_size, sequence_length=128, input_size=128, 채널=1)\n",
        "inputs = layers.Input(shape=(sequence_length, input_size, 1))\n",
        "\n",
        "# 4차원 텐서에서 채널 차원 제거하여 (batch, 128, 128) 형태로 변환\n",
        "x = layers.Reshape((sequence_length, input_size))(inputs)\n",
        "\n",
        "# stacked RNN 구성: 3개의 SimpleRNN 레이어 순차적 연결\n",
        "for i in range(num_layers):\n",
        "    # 마지막 RNN 레이어는 마지막 time step의 은닉 상태만 반환(return_sequences=False)\n",
        "    return_seq = (i != num_layers - 1)\n",
        "    x = layers.SimpleRNN(hidden_size, return_sequences=return_seq)(x)\n",
        "\n",
        "# 마지막 RNN 출력에 fully connected layer 연결하여 클래스 확률 출력\n",
        "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "# 모델 생성\n",
        "model2 = models.Model(inputs, outputs)\n",
        "\n",
        "# 모델 컴파일: Adam 옵티마이저 + 다중 클래스 교차엔트로피 + 정확도 평가\n",
        "model2.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 모델 구조 요약 출력\n",
        "model2.summary()\n",
        "\n",
        "\n",
        "# 4) 콜백 설정: 학습 과정 제어\n",
        "\n",
        "earlystop = callbacks.EarlyStopping(\n",
        "    monitor='val_loss',          # 검증 손실 기준 조기 종료\n",
        "    patience=5,                  # 5회 연속 개선 없으면 종료\n",
        "    restore_best_weights=True    # 최적 가중치 복원\n",
        ")\n",
        "\n",
        "checkpoint = callbacks.ModelCheckpoint(\n",
        "    'best_model.keras',          # 모델 저장 경로\n",
        "    monitor='val_accuracy',      # 검증 정확도 기준 저장\n",
        "    save_best_only=True          # 가장 좋은 모델만 저장\n",
        ")\n",
        "\n",
        "reduce_lr = callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',          # 검증 손실 기준 학습률 감소\n",
        "    factor=0.5,                 # 학습률 50% 감소\n",
        "    patience=2,                 # 2회 연속 개선 없으면 감소\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# 5) 모델 학습 실행\n",
        "\n",
        "steps_per_epoch = math.ceil(train_generator.samples / batch_size)      # 총 학습 샘플 수에 따라 epoch당 스텝 계산 (올림)\n",
        "validation_steps = math.ceil(val_generator.samples / batch_size)       # 검증 샘플 기준 스텝 계산 (올림)\n",
        "\n",
        "history = model2.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_generator,\n",
        "    steps_per_epoch=steps_per_epoch,       # 한 epoch당 학습 스텝 수\n",
        "    validation_steps=validation_steps,     # 검증 시 스텝 수\n",
        "    callbacks=[earlystop, checkpoint, reduce_lr]  # 콜백 적용\n",
        ")\n",
        "\n",
        "\n",
        "# 6) 학습 결과 시각화\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# 손실 곡선 시각화\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Loss Curve')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# 정확도 곡선 시각화\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.title('Accuracy Curve')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ]
}